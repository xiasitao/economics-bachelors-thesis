% !TeX root = scaffold-40.tex
\renewcommand{\imagepath}{../30-algorithms/img}

\chapter{Natural Language Processing Algorithms}\label{ch:algorithms}
In this chapter, the \gls{nlp} algorithms and language models used for grouping and labelling the celebrity newspaper articles are introduced. After a short historical overview of \gls{nlp}, clustering-based and pre-trained model-based language models relevant for the analyses in thesis are presented.


\gls{nlp} has been a field of research in linguistics and computer science since the 1940s. After the first steps of analyzing natural language with machines had been undertaken with rule-based schemes, the advent of fast computer technology in the 1990s and 2000s has given rise to machine learning-based \gls{nlp} algorithms exploring language statistically~\autocite{jurafsky_speech_2008}. The invention of clustering-based language analysis falls into this time. With computation getting even more efficient and ever more powerful artificial intelligence conquering the field of machine learning, deep learning-based language models took the stage as record setters in terms of language modelling performance and quality~(\cite{vajjala_practical_2020}).


\section{Clustering Algorithms}\label{ch:clustering_algorithms}
The goal of clustering algorithms is to assign data points to multiple groups based on a distance measure, such that points that are close to each other are likely to be inside the same cluster, whereas data points distant from each other rather find themselves in different clusters~\autocite{everitt_cluster_2011,jain_algorithms_1988}. Clustering is an unsupervised machine learning technique as it does not require labelled data or any other kind of prior knowledge to work. Rather than inferring regularities statistically, as would be the case for supervised methods, it structures the data as is~\autocite{xu_survey_2005}. The following two clustering algorithms were used in the analyses of this thesis.

\paragraph{Topic Modelling with Latent Dirichlet Allocation}\label{ch:lda}
Topic modelling with \gls{lda}\footnote{The abbreviation LDA is also used for the dimensionality reduction algorithm \textit{linear discriminant analysis}, which is not meant by LDA in this thesis.} is a probabilistic clustering algorithm from the early 2000s used in \gls{nlp}. It aims at finding groups within a corpus of natural language documents, the so-called topics, that are characterized by a set of keywords from the corpus which are most prominent within the topic. It assigns to every document probabilities of pertaining to each single topic, differentiating it from similar clustering approaches just assigning one topic~\autocite{blei_latent_2003, blei_probabilistic_2012}. The arguably most important hyperparameter of \gls{lda} is the number of topics that the algorithm should distill from the provided corpus. In this thesis, \gls{lda} is used to find the prevailing topics among the celebrity news articles and partition them by these topics. The article distribution across the topics is then compared for the two \gls{ses} groups.

\paragraph{k-means}
The k-means clustering algorithm is more generally applicable to any sets of data points represented in a vector space, with the idea dating back more than half a century. It aims at minimizing the aggregate absolute vector space distance of all data points in a cluster from the cluster mean, hence the name k-means~\autocite{macqueen_methods_1967}. As an alternative to \gls{lda}, k-means is used in this thesis for partitioning the celebrity newspaper articles by their embeddings, which are introduced in the following paragraphs.



\section{Pre-Trained Semantic Embeddings}\label{ch:pretrained_algorithms}
When using clustering algorithms like \gls{lda} for modelling a corpus of documents, the model will depend only on the data of the corpus, hence relevant contextual information that is not present in the corpus itself is not part of the modelling procedure. 
In contrast to that, pre-trained context-aware language models based on deep learning, which have emerged just over the last years, have a much more refined capability of dealing with textual context and connotation outside of a specific use case.

\paragraph{BERT and Sentence-BERT}\label{ch:sentencebert}
BERT, standing for bidirectional encoder representations from transformers, is such a language model. It is pre-trained on books and Wikipedia on the task of finding the correct word for a gap in a given context of words. Its use is, however, not constrained to what it is pre-trained on. By fine-tuning it, BERT can also engage in tasks like paraphrasing and token identification~\autocite{devlin_bert_2019}. In this thesis, BERT's capability of providing a context-aware vector representation of meaning, called \textit{semantic embedding}, is exploited. To this end, a derivative of BERT, Sentence-BERT~\autocite{reimers_sentence-bert_2019}, is used in order to get a contextual vector representation of entire documents rather than single words. Note that the components of these vector representations do not bear any intuitive meaning, but their vector space distance encodes their semantic proximity, making them suitable for k-means clustering.

\paragraph{BART and Zero-Shot Classification}\label{ch:zero_shot}
A generalized BERT derivative, BART~\autocite{lewis_bart_2020}, is used in this thesis for zero-shot classification~\autocite{huggingfacebart-large-mnli_facebookbart-large-mnli_nodate,davison_zero-shot_2020}. In \gls{nlp}, zero-shot classification means assigning a label from a predefined set of labels to a document without any further training of the classifier model on the corpus of documents. If the model is pre-trained on semantic similarities, it is already capable of deciding which label is closest to the text in terms of meaning. In comparison to topic modelling approaches, this opens up the possibility of classifying texts even if the labels characterizing the candidate classes are not explicitly part of the texts, but their meaning is present in the context, such as for emotions and connotations~\autocite{yin_benchmarking_2019}. In this thesis, BART-based zero-shot classification is used to explore the celebrity newspaper articles beyond the article topics that clustering algorithms can find.

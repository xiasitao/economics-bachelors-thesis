% !TeX root = scaffold-50.tex
\renewcommand{\imagepath}{../50-unsupervised/img}
\newcommand{\ntopics}{n_\text{topics}}
\newcommand{\nclusters}{n_\text{clusters}}

\chapter{Unsupervised Analysis: Topic Modelling}
In this chapter, the topics of the celebrity newspaper articles are examined using the topic modelling technique. After showing that using \gls{lda} alone does not yield robust and reliable results, semantic-similarity clustering is shown to improve the quality of the topic extraction.  Finally, the significance of differences in the topic distributions of low-\gls{ses} and high-\gls{ses} newspaper articles are discussed.

The main focus is to find the prevailing topics in the celebrity newspaper articles and to examine any differences in the distribution of these topics between the low-\gls{ses} and high-\gls{ses} groups.\todo{Motivate more again}

\section{Shortcomings of LDA Topic Modelling}
In a first attempt, \gls{lda} model instances (see chapter~\ref{ch:lda}) were trained for a varying number of topics $\ntopics$. The corpus of all relevant newspaper articles comprised the 50-article balanced data set ensuring that role models with many articles could not induce a topic bias in the models. The texts were fed to the models in the \textit{noun and verb} stage in order to focus on the meaning-bearing elements of the texts.

Each of these model instances yielded a set of $\ntopics$ topics. Each topic $t$ is defined by a list $T_{\ntopics, t}$ of characterizing topic words. Each model assigned to every newspaper article $i$ estimated probabilities $p_{i, t}$ that article $i$ would belong to the topic $t$. Each newspaper article $i$ was assigned a topic $t_i$ by selecting the topic with the maximum probability:
\begin{align}
    t_i = \arg \max_{t'} p_{i, t'}
\end{align}

Figure~\ref{fig:topic_modelling_schema} illustrates the modelling and assigment process.
\begin{figure}
    \centering
    \includegraphics[]{\imagepath/topic_modelling_schema.pdf}
    \caption{Schematic of the topic modelling setup with \gls{lda}: Many models for different numbers $\ntopics$ of topics are trained, each of them yields a list of topics that are characterized by topic words, as well as topic probabilities for each article.}\label{fig:topic_modelling_schema}
\end{figure}

The \gls{lda} algorithm depends on a set of hyperparameters, most of which are related to its underlying optimization process.  Sensible values for these were found by trial and error. The one hyperparameter that is crucial for the interpretation of this analysis is $\ntopics$. It determines the granularity of the topics the model provides for the comparison of the low- and high-\gls{ses} groups. The trade-off between a low and a high number of topics is expected to be a well-graspable set of topics with considerable overlaps and therefore poor specificity for low $\ntopics$, in contrast to highly delimited niche-topics suffering from a lack of generality for high $\ntopics$. For choosing an appropriate $\ntopics$ in this work, it was required that the topics assigned to articles be consistent for small variations of $\ntopics$, since only then it can be assumed that the assignment of the articles to the low- and high-\gls{ses} groups will not just be a random effect.\todo{clearer} The search for an optimal number of topics included introducing of so-called \textit{hypertopics} to make the topics comparable as well as assessing ranges for $\ntopics$ within which the hypertopic accuracy and the \gls{ses} assignment were consistent.

\paragraph{Assigning Hypertopics}
As the model output topics are only defined by a list of characterizing words, the individual topics can overlap in term of their meaning (e.g. different kinds of sport), and the topics obtained for different $\ntopics$ cannot be compared without human intuition. In order to still be able to compare the model outputs for different $\ntopics$, each topic was manually assigned one of the four \textit{hypertopics} movie, music, sport, and life (with life being intended as a miscellaneous category for all topics not covered by the other three hypertopics). These four hypertopics were heuristically selected while examining the model output topics. Each article $i$ with topic $t$ was assigned a hypertopic $h$:
\begin{align}
    h_i = h(t_i)
\end{align}
Table~\ref{tab:hypertopics} shows examples of the model output topics and their manually assigned hypertopics and shows the generality vs specificity trade-off in terms of the chosen $\ntopics$.

\begin{table}
    \centering
    \begin{tabular}{clc}
        \toprule
        $\ntopics$ & topic words for exemplary topics (model output) & hypertopic \\
        \toprule 
        \multirow{3}{*}{5} & love video fan people thing song feel music share life & \textit{life}\\
        & game player team season goal win league sport club score & \textit{sport}\\
        & film llc st ave dr series movie season actor michael & \textit{movie}\\
        \midrule
        \multirow{3}{*}{50} & music artist business sony continue record label company team partner & \textit{music} \\
        & inc property water art road town science management development site & \textit{life} \\
        & fc week injury season run football quarter marcel yard dortmund & \textit{sport} \\
        \bottomrule
    \end{tabular}
    \caption{Example of model output topics and their respective hypertopics. Comparing the \textit{sport} hypertopic examples for $\ntopics = 5$ and $\ntopics = 50$ shows how a larger amount of model topics makes the topics more specific, here by referencing a particular sport and a particular sports club.}\label{tab:hypertopics}
\end{table}

With every article associated to a hypertopic, it was possible to compare the distribution of articles and \gls{ses} groups across hypertopics for different $\ntopics$, as well as to verify how accurately the topic models had assigned topics to the articles using the human-annotated data.

\paragraph{Accuracy and Consistency}
The accuracy of the hypertopic assignment was calculated as the ratio of articles with correct hypertopic assignment by the model and all articles that were human-annotated:
\begin{align}
    acc = \frac{
        \left|
            \left\{i\middle|h_{i}^\text{predicted} = h_{i}^\text{true}\right\}
        \right|
    }{
        \left|
            \{{i|i \text{ was human-annotated}}\}
        \right|
    }
\end{align}

Figure~\ref{fig:accuracy_by_ntopics} shows that the accuracy greatly depends on $\ntopics$ with a maximum of abount \SI{50}{\percent} at $\ntopics=15$.

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{\imagepath/acc_by_n_topic_modelling.png}
    \caption{Accuracy of the model-assigned hypertopics compared to human-annotated data as a function of the number of topics: A maximum accuracy of around \SI{50}{\percent} was achieved for $\ntopics=15$.The accuracy not consistent across the different numbers of topics.\todo[inline]{replace}}\label{fig:accuracy_by_ntopics}
\end{figure}

Additionally, the share of low-\gls{ses} articles for each assigned hypertopic was used as a measure for topic assignment consistency across $\ntopics$. This share is plotted in figure~\ref{fig:lowshare_per_ntopics}. The fact that there is no range of $\ntopics$ where this share is consistent for small variations of $\ntopics$ suggests that the assignment of articles to the hypertopics is to a considerable extent random and that the model is incapable of making a reliable prediction of an article's topic.\todo{Explain more why you took exactly this share and not anything else.} These observations did not differ significantly if the \textit{distinct-\gls{ses}} set of articles was used instead.

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{\imagepath/low_ses_by_n_topic_modelling.png}
    \caption{Share of low-\gls{ses} articles in each hypertopic across different numbers of topics: The share is not consistent over a range from \SI{5}{} to \SI{50}{} topics, indicating that the assigment of articles to topics is not very reliable. The share of low-\gls{ses} articles among all articles is shown as a reference.\todo[inline]{replace}}\label{fig:lowshare_per_ntopics}
\end{figure}

Two potential approaches for improving accuracy and consistency were examined: Firstly, it was attempted to train the topic models on the low-\gls{ses} and high-\gls{ses} corpora separately, which, however, led to similar inconsistencies. Secondly, it was tried to filter out articles for which the model's topic assignment was ambiguous. As a measure for the ambiguity in topic assignment for article $i$ with topic probabilities $p_{i, t}$, the information theoretical entropy was used:
\begin{align}
    H_i = \sum_t p_{i, t} \ln p_{i,t}
\end{align}
The entropy is low if the probability distribution clearly favors a specific topic, and it is high if the probabilities $p_{i, t}$ are rather ambiguous. After filtering out articles where the $H_i$ is higher than the \SI{50}{\percent}- or \SI{30}{\percent}-percentile of all articles, high accuracies of up to \SI{70}{\percent} were reached, however the consistency problem persisted and due to filtering out articles some hypertopics were not present among the remaining articles at all in some cases.

The author guesses that one of the reasons for the inconsistent variations in topic assignment is the newspaper articles operate on a very limited set of topics, roughly covered by the four hypertopics, which do expose considerable overlaps (e.g. movie-related articles having similarity to life stories, film music-related articles talking about movies). This could render the topic models incapable of capturing distinct groups of articles, rather than assigning topics seemingly at random.

As the quality of the topic modelling approach could not be substantially improved without the scope of \gls{lda}, the modelling process was enhanced by adding pretained transformer model providing semantic embeddings as described in the next section.

\section{Improvement with a Pretrained Model}
In order to mitigate the issue that the \gls{lda} topic model was apparently incapable of consistenly separating articles by their meaning, a pretrained semantic embedding model was added to the modelling process. In this way, the process of finding the topics

Each article was assigned a vector representation by the pretrained SentenceBERT model. These representation were then clustered by the KMeans algorithm, with each cluster representing one topic. In this way, the process of assigning topics to each article did not depend on the newspaper article corpus anymore but instead only on a pretrained model. \gls{lda} with $\ntopics=1$ was then used to assign a single set of topic words to each of these clusters, such that in the end the output of the modelling process had the same structure as for using \gls{lda}. Figure~\ref{fig:embedding_modelling_schema} illustrates the modelling process in this scheme.

\begin{figure}
    \centering
    \includegraphics[]{\imagepath/embedding_modelling_schema.pdf}
    \caption{Schematic of the topic modelling process with SentenceBERT semantic embeddings and KMeans clustering: The article texts are transformed into embeddings, a vector representation of text, by the pretrained SentenceBERT transformer model. Then the KMeans algorithm clusters these embeddings into $\nclusters$ clusters of articles. Each of these sets of articles represents a topic whose topic words are found by separate \gls{lda} models (with $\ntopics=1$) for each cluster.}\label{fig:embedding_modelling_schema}
\end{figure}

Again, hypertopics were assigned to each set of topic words for comparing and assessing consistency across $\nclusters$. With this enhanced approach, more consistency in varying $\ntopics$ was achieved in terms of hypertopic accuracy and the portion of low-\gls{ses} articles per hypertopic, as can be seend from figures~\ref{fig:accuracy_by_nclusters} and~\ref{fig:lowshare_by_nclusters}. As the highest accuracy was achieved with $\nclusters=15$, this choice was kept for the following analysis. 

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{\imagepath/acc_by_n_embeddings.png}
    \caption{Accuracy of the  enhanced model-assigned hypertopics compared to human-annotated data as a function of the number of topics: The accuracy is higher and more consistent across the different numbers of clusters compared to the \gls{lda} approach. A value of $\nclusters=15$ seems promising.\todo[inline]{replace}}\label{fig:accuracy_by_nclusters}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{\imagepath/low_ses_by_n_embeddings.png}
    \caption{Share of low-\gls{ses} articles in each hypertopic across different numbers of topics: The share is comparably consistent around $\nclusters=15$ for all hypertopics, making it a promising choice. The share of low-\gls{ses} articles among all articles is shown as a reference.\todo[inline]{replace}}\label{fig:lowshare_by_nclusters}
\end{figure}

\section{Discussion}